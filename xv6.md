# xv6

`fork()`父进程创建子进程时，子进程**复制**父进程的内存空间、堆栈、代码、数据等，在另一块内存区域保存。**两者并不共享内存区域**  

- 当我们采用fork时，子进程并不会一开始就复制父进程的内存。相反会共享内存空间，当一个进程尝试修改共享的内存时，操作系统才会真正复制。-->虚拟内存技术 (copy-on-write)

`wait()`用来等待子进程结束，获得子进程退出状态返回给父进程。若没有子进程退出**wait会一直等待，知道有一个退出为止**。如果父进程不关注子进程退出状态就采用`wait(0)`  

`exec()`将内存中原有代码数据被新程序取代-->意味着exec允许一个进程在运行时切换到不同的程序

`close()`关闭进程中某个文件描述符，让其可以复用 

`dup()`创建一个现有文件描述符的副本，允许多个文件描述符引用相同的文件或资源，**关闭其中一个并不会影响另一个** ，共享文件偏移量

- `dup()`会自动将副本赋值给当前进程的**最小未使用**文件描述符

**文件描述符表中每一个条目都包含了文件指针、文件状态、偏移量、引用计数等。而文件描述符只是其索引**

- 0-->标准输入
- 1-->标准输出
- 2-->标准error

**pipe实质上是小的内存缓冲区**，暴露给进程两个文件描述符，一个往进写，一个往出读

- fd[0]-->从管道读
- fd[1]-->往管道写


```cpp
int p[2];
char *argv[2];
argv[0] = "wc";
argv[1] = 0;
pipe(p);
if(fork() == 0) {
	close(0);
	dup(p[0]);
	close(p[0]);
	close(p[1]);
	exec("/bin/wc", argv);
} else {
	close(p[0]);
	write(p[1], "hello world\n", 12);
	close(p[1]);
}
```

其中close(p[0]),close(p[1])是因为p[0]已经被复制到 0 文件描述符了 然后关闭子进程中管道的读端写端 这样子进程就可以写入管道

`Mknod()` 创建的是一个设备文件

当进程打开一个设备文件时，内核会将 read write 转移到内核实现

`link()` 系统调用创建另一个文件的名称 两者共享一个inode 实现如下：

> ```c++
> open("a",O_CREATW|O_WRONLY);
> link("a","b");
> ```



**`cd` 命令并不像其他命令一样创建一个子进程 **

***

## Lecture 1:

文件描述符为每个正在运行的程序提供内核中索引

当使用 fork 创建一个进程时 在父进程中返回子进程的pid 在子进程中返回 0 代表子进程正在运行

平常在shell中输入命令 其实就是shell进程创建了一个子进程 来执行命令程序

fork 和 exec通常结合使用 是因为需要父进程和子进程执行不同程序 exec可以将复制过来的父进程代码转换成自己需要运行的

***

## Lab 1：

pipe是半双工的，所以当父子进程要进行相互通信时，必须要使用两个。使用一个会造成竞态条件，例如两个进程同时往里写同时往外读。

> 管道没有锁机制，因此如果在这种情况下使用管道，需要自行设计管道管理

关于**primes lab**：

1. 在使用pipe时，记住pipe是个缓冲区，可以通过循环一个个写入。并不用直接全部输入进去。等到传输的东西都输入进pipe后，再fork用子进程去读取。写完记得关闭写的文件描述符。
2. exit(0)与wait(0)前者是终止当前进程，后者是让父进程等待一个子进程的结束。父进程会阻塞在这里，直到有一个子进程结束。
3. 若当前进程没有创建子进程，当使用wait()时，就会立刻返回，不起任何作用

关于**find lab**:

1. stat()是一个路径指定的文件信息，fstat()则是通过一个文件描述符来查看文件信息
2. 养成用完文件描述符就关闭的习惯
3. memmove()内存复制的标准库函数`memmove(p, de.name, DIRSIZ)`将de.name所指复制到p所指空间中，复制DIRSIZ大小

关于**xargs lab**:

1. `echo hello too | xargs echo bye`在这句命令中，xargs接收到的argc=3 分别是 xargs名本身 以及 echo 和 bye。所以有了下面这段代码:
```c++ 
for(int i=1;i<argc;i++){
     args[i-1]=argv[i];
    }.....
 ..... 
	args[argc-1]=buf;//让传过来的参数附加到参数列表里
        if(exec(argv[1],args)<0){//argv[0]是xargs本身 
           fprintf(2,"exec: error\n");
    }
```

2. xargs是从**standard input**中读取数据 也就是标准输入 而不是standard output!

***

操作系统必须满足三个要求：**multiplexing(多路复用)，isolation(隔离),interaction(交互)**

系统调用system call就是抽象了计算机资源的接口，实现了应用程序和硬件的隔离

为了实现application之间的隔离，即一个崩溃了不影响其他运行，cpu提供了硬件支持。RISC-V有三种cpu执行的模式：**machine mode,supervisor mode, user mode**. application想要调用内核函数通过系统调用,就必须切换到内核。这时cpu会通过一个指令从user mode 转换到 supervisor mode(在RISC-V中是ecall)，**并在内核指定入口点进入内核**，切换mode后，内核决定是否进行执行application的执行请求。

> 系统调用和内核函数的区别：系统调用是application访问内核功能的接口，而内核函数是在操作系统内核汇总的函数，执行内核功能例如进程管理、内存管理等。**内核函数通常是内核内部使用的函数，而系统调用是用户空间应用程序使用的接口**

**Monolithic Kernel** 单体内核，即整个操作系统都在内核中，所有的系统调用都以supervisor mode 执行。（windows ，linux,macOS都是这种设计）

> 在单体内核模式下，若某一个系统调用出错，因为此时cpu是supervisor mode 拥有绝对的所有权，因此可能会导致整个内核崩溃。为了减少这种风险，人们减少了在supervisor mode下运行的代码量，将大部分代码在user mode下运行

以上想法就是**Microkernel**，将核心功能分成小模块，通过消息传递等机制进行模块间通信

每一个进程都有一个执行线程，专门用来执行指令。执行线程挂起时，相关状态保存在线程自己的栈中。

每一个进程都有两个栈，一个 user stack 一个kernel stack。当执行用户指令时，使用user stack，反之，进程进入内核时，代码就会在kernel stack上执行。

**xv6的启动和第一个进程执行过程：**

1. 当RISC-V计算机通电时，先初始化自己，然后运行在read-only memory里的加载程序。加载程序将xv6的内核加载到内存，在machine mode下，在`_entry`处开始执行xv6。（在这里RISC-V为了引导简化和初期启动要求禁用了分页硬件，由虚拟地址直接映射到物理地址）
2. 加载程序将xv6内核加载到物理地址较高的地方，在本文中是0x80000000.原因是低地址通常保留用于系统的重要部分，比如I/O映射。
3. 在machine mode下进行一些配置，然后切换到supervisor mode，RISC-V提供`mret`指令，该指令原本意为从machine mode 切换为原来的特权级别。而在这里，它执行的是将mstatus中设置为supervisor, 将main()的地址写入寄存器以作为返回地址，将所有中断和异常授权于supervisor mode. 配置定时器以产生定时器中断，将程序计数器设置为main()的地址，以开始执行操作系统内核主要代码。
4. 在main()初始化一些设备和子系统后，例如page table,cache等，开始创建第一个进程，第一个进程执行RISC-V中的exec，执行用户空间的`/init`,这个程序会创建一个新的console device file 然后将其文件描述符 0 1 2打开，在上面启动一个shell

***

## Lecture 2:

**所有的处理器，若需要运行能够支持多个app的操作系统，那么它就需要同时支持user/kernel mode和虚拟内存**

application cannot crush the OS，app不能打破自身隔离 因此OS要被设计为防御性的 这意味着**app和OS之间应该有强隔离性** <----硬件支持（user/kernel mode或者 virtual memory）

页表实现从虚拟地址映射到物理地址，每一个进程拥有自己的页表-->强的内存隔离

在RSIC-V中，我们在用户空间直接使用fork()创建子进程背后逻辑是：编译时通过ecall指令切换到kernel, 而在kernel中会有一个syscall.c文件通过寄存器读取要调用的系统调用，再执行内核中的fork()进行创建

**在单核场景下，单个断点就可以停止整个程序的运行**

***

## Lab 2：

整个Lab 2是让我们熟悉系统调用在xv6系统中的执行过程。

关于**trace lab**：

1. 通过在用户空间中user.h中**声明**我们的跟踪函数 `int trace(int);`
2. 编译时，通过perl脚本生成RISC-V汇编语言，将SYS_trace存入寄存器a7(a7寄存器在xv6系统中专门存储各种系统调用的号码)。再通过`ecall`指令进入内核状态！！
3. 在内核状态，会跳转到`syscall()`系统调用通用入口，去找到对应系统调用函数。从寄存器a7中读取出号码，与系统调用的表对应执行系统调用函数。返回值存在寄存器a0里。
4. 执行完毕后，利用`argint()`从用户空间获得参数，若此时执行的系统调用在所获取参数范围内，按照要求打印这次系统调用的信息。
5. 从`ecall`中结束，通过`ret`指令返回用户空间

关于**sysinfo lab**:

1. 声明的是一个结构体指针，如果要对其成员进行赋值，需要对这个指针给予空间，否则会导致内核崩溃。要么赋予空间，要么直接申请结构体变量而不是结构体指针。
2. xv6内核，采用的是一个单链表管理空闲内存，每一页是4096 bytes

***

页表用于将程序的虚拟地址映射到**物理内存**中的实际物理地址。（是物理内存而不是物理硬盘）

**页表是用来实现虚拟内存系统的**，虚拟内存的优点在于：

1. 允许程序使用比物理内存更大的地址空间
2. 允许多个程序并发运行，每个程序都认为有它自己的完整内存

RISC-V指令（包括用户级别和内核级别）操作的是虚拟地址。而RAM(随机访问存储器)、物理内存都是通过物理地址来索引。而连接这两种地址的就是页表。

**页表映射过程**：

![页表映射过程](./page table map.png)

这里是以RISC-V64bits作为例子，虚拟地址中只有bottom 39bits 被用来映射。其中0-12bits 是在当前块里的**偏移量**，12-39 也就是27bits是用来在page table中查找的(**共能表示/查找`2^27` 个PTE**)。在pagetable中找到对应的44bits **PPN**(physical page number)，44bits的PPN和10bits的flags构成一个**PTE**(page table entries). 这里的flags中包含存在位、读/写权限位、访问位等等。**最后这44bits的PPN和虚拟地址中12bits的偏移量拼接成56bits的物理地址。**

**多级页表**：

![](./multi-level page table map.png)

这个例子就是将原来的27bits寻址划分成了三块，前9个在根页表里找到次级页表的位置，中间9bits在次级页表里找到final 页表的位置，末9位在final 页表里找到最终的构成物理地址的PPN. 

> **为什么要用多级页表**？
>
> **节约存储页表而耗费的内存页**。页表也是存储在物理内存中，像之前27bits的页表表示的是`2^27`个条目，可能页表太长导致一个物理块存储不下。因此就进行多级页表来优化内存管理，通过将页表划分为多个级别，每一个级别负责一部分地址的范围，有效减少单个页表的大小。

要告诉硬件使用页表，内核必须将页表的物理地址写入 **satp 寄存器**。**每一个CPU都有属于自己的satp寄存器**，也就是说，CPU用自己的satp指向自己的页表，来翻译后续生成的所有地址。

> 不同CPU的对应不同的pagetable，但是都映射在同一片物理内存。因此，会出现多个CPU共享同一块物理内存区域，以便他们可以访问相同的数据，这对于在多核CPU上**运行多个任务**以及进行**多线程编程**十分重要

**每一个进程都会有属于自己的页表**，保证不同进程之间的地址空间隔离。当这个进程被cpu执行时，这个进程的页表就会被加载进cpu的satp寄存器。这个工作由操作系统在进程切换时自动完成。

**内核地址空间**，其实是一个概念，它与物理内存之间的关系是，内核需要被加载到物理内存中执行，所以内核地址空间本质上还是属于虚拟地址空间，不过它与物理内存之间的映射是**直接映射**:

![](./kernel_address_space.png)

也会有个别不是直接映射，比如`trampoline page`与`kernel stack page`

RISC-V的每一个CPU将 page table entries 缓存在**TLB**(Translation Look-aside Buffer)中，当切换页表时，必须告知CPU之前的TLB中的缓存页表无效了。

进程**用户地址空间**：（里面不包含了kernel space）

![](./process_address_space.png)

>  最上方是**用一页映射了trampoline code**，这使得这个单独的页面对应的物理空间在所有进程地址空间中都可见，用来实现内核和用户之间的切换。
>
>  其中stack是单独的一个页面，由上至下分别是命令行的参数以及指向他们的数组指针。紧接着下面的是main函数的地址。
>
>  xv6在用户堆栈下方防止一个保护页，检测用户堆栈是否溢出，如果用户访问了堆栈下方的地址，硬件会报页面错误异常，因为在这里的页面并没有实际对应着内存地址。

***

## Lecture 4：

 **CPU无论执行什么指令，只要带有地址，其地址就是虚拟地址。**

**MMU**(Memory Mannagement Unit)负责在硬件层面上执行地址转换，页表是维护那种映射关系。

> MMU不存储page table，page table存储在内存中。因此当CPU处理虚拟地址时，MMU会进行一次访存。

以64bitRISC-V为例，它的内存地址长度为56（44+12，44是映射找到的物理地址前半部分，12是偏移量）。是设计师们按照现有芯片的发展趋势规定出了55，每一块的大小是4096（$2^{12}$）才顺势得到的44。

**多级页表会提高内存利用率：**

> 多级页表结构只为实际使用的部分分配页表项，未使用的不分配，因此节省了内存空间。以**三级页表结构和单级页表作为对比**：
>
> 若一个进程所占用内存空间只有1页的大小，单级页表结构创建的页表会有$2^{27}$个页表项，而三级页表结构只有3*512个页表项就可以索引到该物理页面。前者需要$2^{27}/512$个物理页面来存储，而后者只需要$3*512/512$也就是3个页面来存储即可。

多级页表中，页目录表中所记载的都是**物理地址**了，不是虚拟地址，我们要在内存中找到下一级页目录。

多级页表中，在第一级目录寻找第二级目录时，（物理地址应该是44+12位）,但目录中的是44+10位（44bits的PPN,10bits的flags），第二级目录的地址是这里的**44bitsPPN后边跟着12个0.**

 **三级页表结构是硬件实现的，OS只是往里面填充数据。**

当我们kvinit()时，已经将内核页表首地址加载进了satp，以此开启了之后处理的地址都是虚拟地址的操作，又因为之前我们说到三级页表中前两级PPN存储的是下一级页表的物理地址，那么此时CPU处理不会将这个地址看做虚拟地址进行转换吗？**不会！！！**因为内核的地址空间都是采用的**直接映射**，就算转换了也是真实的物理地址

每一个进程都会有自己的用户栈和内核栈，用户栈存储函数调用的返回地址，局部变量等，在进程的虚拟地址空间中。内核栈是进程在内核态时，使用的栈，执行系统调用或者发生中断切换到内核态使用，存储在内核空间中。

***

## Lab 3：

**这个实验让我们搞清楚的是**：每一个进程都有自己的用户地址空间（用户态页表维护），共享一个内核地址空间（主内核页表维护，这样做优势在于进程可以共享OS提供的各种服务与资源）。因此往常进程切换，只是用户页表被切换走，而内核页表不会变。但现在题目要求我们，将每一个进程都维护一个内核页表，那么也就是让我们将进程共享的那个主内核页表copy给每一个进程，让它们每一个都可以通过自己的内核页表访问到OS提供的服务。（xv6的进程中进程只有用户态页表维护，我们所使用的OS几乎都是进程共享部分内核地址空间）

1. 所以，这个实验有两种解决方案，一种是**copy**，给每一个进程都copy一份内核页表，另一种是**share**，所有进程都share同一部分内核地址。
2. **每一个用户进程都会维护一个内核栈**，存储函数调用的上下文信息以及中断和异常处理的上下文信息等。在xv6中，进程的内核栈在系统刚开始启动时，就会将该cpu能容纳的每一个进程的内核栈都做好映射。
3. 进程的页表被存储在kernel space中，这是为了OS可以更为方便的查找切换不同进程的页表。所以freeeproc中先是释放掉 0-sz 对应的物理页，之后又进行了freewalk 页表。
4. `walk()`返回参数va所对应的PTE(加上offset成为pa）。在OS中，硬件已经实现了3级pagetable的查找，那为什么我们还需要一个相同功能的walk()函数呢？因为：在xv6中，内核是有自己的页表，用户进程有自己的页表，当我们需要在内核状态访问用户进程页表中的va所对应的pa时，（此时，satp中存的已经成为内核页表了，MMU进行mapping的是内核va）就需要用walk()模拟MMU的功能。

实现Lab 3的关键是要搞明白，题目要求我们干什么，xv6原来的设计是怎样的，经过我们的lab后又是这怎样的，特别是vm.c的每一个函数的运行逻辑是怎样的。搞清楚这些，就能对虚拟空间的理解深入很多。

***

 RISC-V与x86-64:

> 后者是我们常见的电脑x86架构，64bits的处理器，前者是精简指令集，数量上较x86少很多。x86的很多指令都不止做了一件事。RISC是市场上唯一一款开源指令集

ARM架构就是RISC的实现。

.asm 与 .S 都是汇编文件。前者是一种通用的汇编文件，不依赖与特定的汇编器或者OS。而后者是特定于Unix/Linux OS的汇编文件。

gdb调试时，(xv6)，pc出现任何0x800的地址都可以在kernel.asm中找到对应的，然后设置断点。

**tui enable**

![](./RISC-V_fncalling_register.png)

我们在使用和谈到寄存器的时候都会用它的ABI名字，原本的名字不重要。

a0~a7用来作为函数参数，若一个function有超过8个参数，就要使用内存来存放参数。

**Saver**列有两个值，Caller Callee,区分这两个的方法是：前者在函数调用的时候不会保存，后者在函数调用的时候会保存。

> 这里的意思是，一个Caller Saved寄存器可能被其他函数重写。假设我们在函数a中调用函数b，任何被函数a使用的并且是Caller Saved寄存器，调用函数b可能重写这些寄存器。我认为一个比较好的例子就是Return address寄存器（注，保存的是函数返回的地址），你可以看到ra寄存器是Caller Saved，这一点很重要，它导致了当函数a调用函数b的时侯，b会重写Return address。所以基本上来说，任何一个Caller Saved寄存器，作为调用方的函数要小心可能的数据可能的变化；任何一个Callee Saved寄存器，作为被调用方的函数要小心寄存器的值不会相应的变化。

![](./stack.png)

黑色框画出来的就是一个**栈帧(stack frame)**, 每一个function都会创建自己的frame并移动**sp(stack pointer)**来使用。**栈是从高位到低位扩张的**，因此我们可以看到入栈的时候sp都进行减操作。每一个stack frame的大小会不一样，但是**fp(frame pointer)**所指的第一个位置永远是return address，第二位置永远是上一个stack frame的位置。fp存在的意义是可以跳转回去。

stack frame 必须要被汇编代码创建，所以是编译器生成了汇编代码，进而创建了stack frame。

**struct在内存中是一段连续地址**，创建一个struct，里面的字段会彼此相邻，可以认为struct像是一个数组(存放方式).

***

导致CPU将指令的执行放到一边，强行transfer处理该事件的特殊代码上的三个事件：1.**system call** 2.**exception** 3.**interrupt**, 以上统一称为**trap**.

**trap的一般流程**：trap force a transfer of control into the kernel-->kernel save register and other state to be resumed-->kernel excete appropriate handler code-->kernel restore saved state returns from the trap--->original code resumes.

多核CPU上的每一个CPU都有自己的一组寄存器，同一时间可能有多个CPU在处理中断。

some impoortant registers:

> **stvec**:kernel将trap handler程序的地址写在这里，RISC-V跳转到这里执行。
>
> **sepc**: 当trap发生时，RISC-V将当前pc的值保留在这里，等待resume(也就是说kernel可以改写这个寄存器的值，让处理完trap回到其他地方)，**sret**会copy sepc to pc.
>
> **scause**:RISC-V在这里保存一个数，指示trap为什么发生
>
> **sstatus**:其中**SIE** bit 控制着设备中断是否可用，也就是在处理trap时，禁止再次发生中断。kernel clear SIE，RISC-V就会延迟设备中断。**SPP** bit指出该trap来自user mode 还是kernel mode。

当RISC-V需要处理一个trap，硬件做的所有事：

![](D:\mit\hw_trap.png)

8.Start excuting at the new pc

在处理trap时，CPU不会自己切换到内核的页表，也不会切换到内核栈，也不会保存除了pc之外的寄存器。**这些工作都需要内核的函数自己完成**。

因为RISC-V硬件不会自动切换页表during a trap，所以user page table必须包含对`uservec`的mapping，在`uservec`中切换satp到 kernel page table。`uservec`也需要在kernel page table中映射到相同位置，确保执行流程可以继续。xv6为了满足这些条件，**设置了`trampoline`page 其中包含`uservec`**。在每一个user page table和kernel page table 中，trampoline page都映射在相同的虚拟地址。

处理trap时，是将**registers中的值保存在内存**或者存储器中，所以先是 sd,当处理完之后，再从存储器中加载到registers ，所以是ld。（xv6中是进程的trapframe保存）

**以system call作为cause梳理Xv6 trap机制**：

* 总分为**7个阶段**：1.从用户态函数到**ecall** 2.执行**uservec**(保存现场，转入内核) 3.执行**usertrap**(判断中转类型，转到对应处理逻辑) 4.执行syscall(调用内核函数，完成系统调用) 5.执行**usertrapret**(设置各种值，为返回做好准备工作) 6.执行**userret**(恢复现场) 7.重返用户态执行后边指令

* 1.ecall是一把钥匙，打开内核服务大门。实质上是ecall主动触发一个用户态异常，导致一系列硬件trap动作：![hw_trap](D:\mit\hw_trap.png)

  这些步骤跟上一个图逻辑是一样的，需要注意，这些动作是**硬件处理trap的自动动作**（硬件电路完成），无论触发trap的原因是什么。

* 2.执行uservec这一阶段与执行userret相互呼应，都是**trampoline.S**里的两个函数，而之前我们从进程的用户地址空间和内核的地址空间，**高地址处都有一页指向这段代码**，因此，这两个函数才可以在用户态和内核态切换之后还能正确识别到地址，正常运行。

  这个阶段就是将寄存器的各种值存在进程的**trapframe**中，这个trapframe是每个进程被创建时分配的一页专门用来存放上下文状态的内存。完了所有寄存器的存储，接下来就要切换到内核态，首先设置内核栈指针(每个进程都有自己的内核栈指针在结构体里存着，只需要把它加载到sp寄存器即可), 再将其余**信息加载到对应寄存器**中。下一步**跳转到usertrap**执行。

* 3.执行usertrap，这一阶段简单概括就是，通过判断这个trap的原因是什么，是syscall，device interrupt，还是exception,分别**转到各自的处理程序**。

* 4.我们以syscall举例，这里就是跳转到调用内核函数完成功能。

* 5.执行userret, 首先设置stvec的值到uservec（ps:会疑问？我都到这一步了才设置stevc的值？那我怎样执行的第二阶段？事实上在fork进程的时候必定会返回这里，在那时就已经设置好了  *fork不也是会调用ecall然后走这个流程么 那当时怎样确定的stevc的值？*）, 设置trapframe中有关内核的值，为下一次的trap做准备，设置sepc，准备调回至原先的地方，调用userret.

* 6.执行userret，恢复现场。将sepc中的值放入pc，将trapframe中存的值加载进寄存器，以及切换回用户页表，通过最后一条指令**sret返回到用户模式**。

* 7.现在已经回到了用户模式，且pc指向的是ecall指令的下一条，执行后续。

trap from kernel space:大体过程与from user space差不多，不同的是，此时OS就处于内核态，一是不用切换页表，不用切换栈，直接对应着找到kernelvec之后就可以开始保存上下文，然后进行kerneltrap，只不过这里的trap只分为device trap和exceptions了。

在我们了解trap机制的时，不难发现，trap from user space处理起来比较麻烦，因为需要考虑到页表的不同，需要重新寻址。因此在**现实世界中，许多OS就会将内核内存的PTE映射到用户页表页面中**，（类似上一个lab做得事情）使其处理trap提高效率。但**Xv6为了避免用户直接操作kernel造成安全bug，取消掉了这样的设置**。

***

## Lecture 6

usertrap()中有一句`p->trapframe->epc=r_sepc()`就是将存在sepc里的pc值又存在trapframe里。那为什么不一开始在存寄存器的时候就直接存进trapframe，而是要在sepc中暂存？

> 因为存寄存器的时候是在trampoline阶段，此时pc是指向uservec的（硬件自动指向），因此为了能进入这个阶段就需要将原来的pc暂存一下，进入了这个阶段之后取出来存。

在RISC-V标准中，**异常**(exception)是当前CPU运行时遇到的与**指令有关**的不寻常情况，**中断**(interrupt)是因为**外部异步信号**引起的控制流脱离当前CPU事件。而**trap表示的则是由异常和中断引起的控制权转移到陷阱处理程序的过程**。

***

## Lab 4

我觉得这个实验以及课程实质上是让我们搞明白，syscall如何让内核trap，以及又如何恢复，这个过程我在上面叙述的很清楚了。但是关于实验，做得却不是很好。

1. backtrace这个实验就是让我们加深stack frame的作用以及结构，当调用一个函数时，首先会为其创建一个stack frame，值得注意的点是，整个栈是由高地址到低地址分布，而sp指的是栈顶也就是栈所在页的中间部分（栈里存有东西）。
2. alarm这个实验是实现一个系统调用，可以让kernel每隔一段时间去执行一个用户级函数，整体做下来，给我的感受就是逻辑性很强，必须要对你要干什么清清楚楚，以及很好理解trap这个过程。

***

##Lecture 8

许多内核使用page fault 来实现写时复制(copy-on-write)。(Xv6目前不是这样)RISC-V有3中page fault：1.**加载页错误**，加载一个页面时，发现其不在RAM中。2.**存储页错误**，向一个va写入数据，但这个va无法翻译。3.**指令页错误**，特定于指令内存(第一个指访问内存时，第三个指执行指令时)。

**copy-on-write**基本计划是，父子进程刚开始共享所有物理页面，但是只将其映射为只读，当父或子进行存储指令时，触发一个page fault。kernel 响应此exception，将**复制包含故障地址的页面**，将一个副本map到子进程空间，另一个副本map到父进程空间。更新页表，恢复错误处理，此时kernel已经更新了相关PTE及权限，因此可以执行。**修复完页表错误后，我们需要重新执行导致错误的指令，并不是像trap一样是指令的下一条**

using page fault, we can 动态的改变mapping。

当page fault发生，我们想要利用其fault做一些事情，就需要足够的信息：1. fault va,存储在stval register。2. type of the page fault，存储在scause register。3. the va of instruction that cause page fault，存储在p->trapframe->epc以及sepc register.

与COW类似的是**lazy allocation**。当一个进程调用sbrk扩充地址空间时，内核会grow address space，但是在其页表中设为无效，当访问到这些new page时，出发exception，kernel这时才会真正给分配内存。

* 在这里我们需要注意sbrk()扩充的是**heap空间**，刚开始指向的是stack的顶端，也就是说，**当一个进程刚被建立时，它是没有heap空间的**，然后通过sbrk()向上增加，对应的p->sz也增加。并且，**sbrk()总是会add over ask**。
* lazy allocation实现了**按需分配**，也就是说，虽然sbrk()让我们p->sz增加了5个page，但实际上我们只为其中1个page分配了物理内存，因此，在umvunmap的时候会出现unmap错误。企图释放没有map的空间。

按需补0，类似于lazy allocation，在user space中BSS段存储着，未初始化的全局变量和静态变量，这里可能会存储着多个page但是内容都是0，那么我们将这些page其实都只是映射到一个全是0的物理page，当某个page需要写入时，触发了pagefault，我们会新创建一个page，内容设为0，重新执行指令。

COW fork, 子进程创建后会与父进程**共享**父进程的物理页面，但此时**两者的权限为只读**，当父或子进程企图store指令时会触发page fault(往一个只读的PTE写数据)，**谁store谁需要拷贝相应的物理page在新的page上**，然后**将其映射到自己的pagetable里，同时将权限改为RW**，另一个进程此时对这个页面的权限也可以改为RW，重新执行指令。

* 我们如何知道这个往一块内存中写入数据，是COW时发生的还是真的一个违法操作？内核需要识别这是一个COW场景，PTE中跟R-X-W-U这些权限在一起的有两位RSW，在这两位中标记为当前page是COW page.
* 父子进程共享了一些物理页，当某一个进程退出时，释放掉物理页，会很麻烦，其实每一个共享的page都会有一个ref计数，每取消的一个共享就ref--

当内存耗尽时，这时还需要allocate page，应该撤回哪些page？采取什么策略？常采用LRU(Least Recently Used)，或者再加上撤回non-dirty page.![PTE_structure](D:\mit\PTE_structure.png)

其中D就是表示是否dirty，A标记是否被访问，就是LRU的实现，RSW就是上文提到的COW场景标记。

***

## Lab 5

这个Lab一共3个小实验，前两个老师在课上已经演示的差不多了，真正需要自己动脑袋的是最后一个小实验，也就是通过给hint完成它所有的test。

我觉得难点在于，我不知道何时会load或store这个还未分配的va然后报错。以及还是没读懂题意，比如最后一个hint是让我们 handle faults on the invalid page below the user stack, 这个含义是在说解决va如果映射到guard page了的错误，因为这时候都是显示物理内存没有被真正分配，但guard page是不用分配的。

又比如，我到现在也没搞明白，为什么在刚开始的时候。shell进程fork然后exec，哪里调用了uvmunmap？

* 突然想明白了，fork完之后，父子进程都是相同的pagetable以及内存，**执行exec需要把子进程的所有都释放掉**(也不是释放，清空？)，然后**赋值**上要执行的程序各种东西。

***

## Lecture 9

之前提到在Xv6内核地址空间中，低于0x8000的都是硬件设备，如果访问的va是这些位置，那么就会访问到不同的硬件设备(主板的设计人员决定)。其中：

* PLIC是中断控制器
* CLINT也是中断的一部分
* UART0负责console和显示器交互
* VIRTIO disk与磁盘进行交互

中断，可以简单理解为**硬件想要得到OS的关注**。产生中断，OS需要保存当前工作，处理中断，然后resume。听起来与**trap的那一套很相似**，是的，大体流程是这样，但还是相比于systemcall引起的trap有点不同：

* **异步性**，硬件产生中断时，中断处理程序和当前运行的进程**在CPU上**没有任何关联。而systemcall是发生在当前进程的背景下。
* 并发性，CPU和**产生中断的设备**是并发执行。
* 对设备编程。

PLIC中断控制器，收到来自设备的中断后，会通知CPU有一个待处理的中断，之后，**一个核**会发出声明，接受了这个中断，PLIC不会将中断发给其他核。CPU核处理完，通知PLIC，PLIC不再保留这个中断信息。

**驱动也就是管理设备的代码**，通常驱动都被设计为两个部分，**top与bottom**，bottom就是中断处理程序，而top部分是其他进程调用的接口。

 RISC-V与中断相关的reg:

* SIE, 有一个bit针对外部设备的中断，一个bit针对软件中断，一个bit针对于时钟中断。
* SSTATUS，中有一个bit来控制中断的打开与关闭。每一个CPU核都有独立的SIE和SSTSTUS寄存器
* **SIE和SSTATUS中的SIE位都是控制中断，有什么区别呢**？简单来说，**SIE reg类似于一个小开关**，控制着那种类型的中断可以进入到supervisor模式。而**SSTATUS中的SIE位则是相当于总开关**，决定是否允许中断或者异常进入supervisor模式。
* SIP, 发生中断时可以看是什么类型的中断
* SCAUSE与STVEC在中断的处理流程中，与之前工作方式一样

看了中断的视频很懵逼，很乱好在找到一位老哥的总结很牛逼：![UART_console_interrupt](D:\mit\UART_console_interrupt.png)

**console的定位就是一个软件抽象出来的设备体，专门用来缓存用户输入的字符，进行预处理，使用UART协议可以正常打印，shell可以正常解析。**

Interrupt相关的并发：

* 设备和CPU是并行运行的。例如当UART向Console发送字符的时候，CPU会返回执行Shell，而Shell可能会再执行一次系统调用，向buffer中写入另一个字符，这些都是在并行的执行。这里的并行称为producer-consumer并行
* 驱动的top和bottom部分是并行运行的。例如，Shell会在传输完提示符“$”之后再调用write系统调用传输空格字符，代码会走到UART驱动的top部分（注，uartputc函数），将空格写入到buffer中。但是同时在另一个CPU核，可能会收到来自于UART的中断，进而执行UART驱动的bottom部分，查看相同的buffer。所以一个驱动的top和bottom部分可以并行的在不同的CPU上运行。这里我们通过lock来管理并行。

***

## Lecture 9

使用lock的principles是：

* 若一个变量可以同时被一个CPU写被另一个CPU读，就需要上锁防止操作重叠

* 锁保护不变量，不变量是在程序执行过程中始终保持真实的条件或属性。例如，如果我们有一个排序的列表，那么列表的排序就是一个不变量，因为无论我们对列表进行何种操作，它都应该保持排序。

  我们有一个链表，我们希望保持链表中的元素总是按照升序排列。这就是我们的不变量。现在，假设我们有多个线程，它们都想在链表中插入元素。如果我们不使用锁，那么可能会出现两个线程同时插入元素，打破了我们的不变量（即链表的排序）。但是，如果我们使用一个锁来保护整个插入过程，那么在任何时候，只有一个线程能够插入元素，我们的不变量（链表的排序）就得到了保护](https://www.cs.columbia.edu/~junfeng/11sp-w4118/lectures/lock.pdf)[1](https://www.cs.columbia.edu/~junfeng/11sp-w4118/lectures/lock.pdf)。

spinlock与interrupt handler：如果一个自旋锁被一个中断处理程序使用，那么CPU必须在中断弃用的情况下永远不持有那个锁。

* spinlock与互斥锁的不同在于：在锁不可用的情况下，互斥锁会使进程睡眠，直到锁可用。自旋锁会进入一个紧密的循环，持续的检查锁是否可用。

* 关于这段话的理解为，interrupt handler是不能被阻塞的，若一个interrupt handler企图获取一个已经被锁定的spinlock，那么就它就会进入无限循环，无法快速响应硬件中断。(进程持有这个锁，但是遇到中断了，进程被挂起，没法释放这个锁，中断处理程序又在等待这个锁，造成了死锁)。因此我们在中断处理程序中使用自旋锁时，我们需要在获取锁之前禁用中断。这样，我们就可以确保当中断处理程序持有锁时，不会发生其他中断，从而避免死锁](https://stackoverflow.com/questions/27151445/interrupt-and-spinlock)[2](https://stackoverflow.com/questions/27151445/interrupt-and-spinlock)

  Xv6更加保守，当CPU获取任何锁时，xv6总是禁用该CPU上的中断。

若有一个进程长时间访问disk,就会长时间持有spinlock，那么这时候若有其他的进程获取这个锁就会忙等待(不停的询问)浪费CPU，也就是说持有spinlock的那个进程不能在访问disk的时候将CPU让出去，造成浪费。那么我们想在此刻让CPU让出去，让其他需要的进程来使用，但锁的类型是spinlock的话，会造成死锁，并且违反了在持有spinlock时，中断必须关闭的要求。此时sleep-lock就出现了。

采用原子指令**获取**锁，Xv6中采用**__sync_lock_test_and_set**中的**amoswap指令**实现

 锁带来的挑战：

* 死锁
* 破坏了程序的模块化(为了防止死锁，一个模块可能需要知道另一个模块中所持有的锁或者持有锁的顺序)
* 锁与性能之间的权衡

***

## Lab 6

这一个Lab 只有一个实验，就是实现COW，我的整体思路是正确的，但是由于没读懂题意，导致不知道如何设置每一个物理页的reference数量。题目已经明确给出提示，设置一个数组来保存即可，其他倒是没有什么特别难的，只要想明白COW的整个过程即可。

***

## Lecture 10

当OS支持线程时，那么**CPU上运行的就只是线程**了，每一个进程会有一个主线程。在这种条件下，我们说**进程运行就是指一个或者多个线程在CPU上运行**。那也就是说，**进程切换其实就是在CPU上进行线程切换**，但一个进程包含多个线程，进行切换和前述不同。

定时器中断（抢占式调度）：将程序运行的控制权从用户空间切换到内核空间，然后interrupt handler接手，然后自愿的让出CPU给线程调度器(yield)。注意：定时器中断会强制的将CPU控制权从用户进程给到kernel，之后kernel中的用户进程对应的内核线程代表用户进程自愿的让出CPU。（本质上还是抢占式，**区分抢占式还是自愿式就是CPU的控制权如何让出**）

进程的状态：RUNNING、RUNNABLE、SLEEPING

在Xv6中，从一个用户进程通过定时器切换到另一个用户进程都需要：

* 首先用户进程P1从用户空间进入内核空间，此时trapframe会保存进入与离开的数据。
* 在trap里通过监测，识别到这是一个定时器中断，进入到yield().
* yield进行加锁，将当前进程的状态调整为RUNNABLE(虽然状态是这样了，但其实还在当前CPU上运行)，然后调用sched().
* sched()主要是调用切换线程的核心函数swtch(), swtch是利用汇编写的函数，主要作用是将当前运行的线程的寄存器值保存在p->context。然后恢复当前CPU核里的context。每一个cpu核都会有一个context对象，存储的就是所属自己的调度器上下文。这里也就是从用户进程P1的内核线程切换到调度器线程。（注意别混淆，并不是CPU的context就是当前运行进程的context，context只是一个存储对象，每一个proc都会有自己的）
* 通过swtch切换成功之后，当前CPU就不是运行P1了，而是调度器线程，它规定的返回地址已经是scheduler()，因为调度器上次切换时，保存的上下文就是scheduler里的swtch，因此这次切换回去，直接从那里醒来。
* 跳转过来后，将当前cpu上得proc设为0，表示当前CPU上无RUNNING进程，需要挑选一个RUNABLE进程运行。接着释放P1的锁，表示这个进程可以被其他CPU所看到。
* 接着从循环里，挑一个RUNNABLE进程P2，先获得其锁，然后设置其状态为RUNNING，再次调用swtch。这次调用swtch，则是在CPU的context中保存了当前调度器线程的上下文，然后加载刚被挑选的进程P2。
* swtch，就是将当前运行的上下文从调度器线程转成了P2，因此其返回地址是之前切换时保存的（之前被定时器中断在这里），sched中，之后一步步返回到yield（在yield中释放掉锁），再返回到trap，在到trapret中，恢复在trapframe保存的进入内核时的数据，返回到用户空间执行

那如果切换到一个新进程上应该怎样？会做一些特殊处理，首先会创建进程的上下文(包括分配内存，初始化进程控制块，设置新的用户栈和内核栈)，加载程序，设置入口点，切换到新进程。

**注意：**这里提到的一个用户线程对应着一个内核线程，这种说法是教授的个人习惯，实际上**一个进程只有自己本身，有时在用户空间执行指令，有时在内核空间执行指令**。教授说就可以理解为进程对应着两个线程，一个运行在用户空间，另一个运行在内核空间，两者不会同时运行。

那么之前一直提到context对象存储在哪里呢？存储在每个进程的结构体里，会有一个context的结构体，里面是一些register。那调度器线程又没有对应的进程，因此没有proc结构体，它的context存储在CPU的结构体中，每个cpu核会有一个结构体。

若CPU是单核，那么多线程任务需要加锁吗？必须加，虽然CPU核只有一个线程在运行，但我们不清楚OS进行切换的时机，若线程a刚修改完某一变量的值，发生了切换，线程b也要修改。运行完之后切换回a,变量的值被篡改为了b里的值。

关于caller-saved寄存器与callee-saved寄存器：我的理解是，前者需要在调用者处保存，等从被调用函数返回后，需要恢复这些register，在函数调用过程中，这些regs可以修改。后者是在被调用者处保存，这些regs不可在调用过程中修改，那既然不能修改，为何又说保存呢？实际上“不能修改是面对调用者说的”当这些regs在被调用方保存了后，然后使用，在返回调用者之前，被调用者会恢复这些regs原有的值，因此在调用者看来，整个调用过程这些regs没有被修改。

***

## Lab 7:

* 三个实验都比较简单，第一个实验搞清楚线程切换的原理即可。第二个第三个实验直接就是体验mutex与condition variable。

***

## Lecture 11：

**sleep和wakeup**系统调用的一些事情：使用这俩个实现进程之间的同步机制(synchronization mechanism)，当一个进程可以唤醒另一个沉睡的进程。

我们通常将同步和**信号量**等价在一起(**semaphore**). 最常见的同步就是“PV”，信号量维持一个计数变量以及两个操作，V操作++变量，P操作--变量。对应着生产者和消费者。这俩分别是两个进程，那么访问这唯一的信号量就需要上锁机制。![PVoperation1](D:\mit\PVoperation1.PNG)

![PVoperation2](D:\mit\PVoperation2.PNG)

这里的sleep操作释放掉了这个**条件锁**并让当前进程状态设为SLEEPING

**wait、exit系统调用底层就使用了sleep和wakeup**，首先需要清楚，exit是进程退出时用的，wait是父进程回收子进程资源使用的。若子进程exit，父进程不wait或者waitpid，那么就会造成**僵尸进程**(ZOMBIE)，虽然子进程不占用内存了，但是它所占的进程ID不会被释放掉，而系统的进程ID是有限的。**如何解决**？1.使用wait或者waitpid 2.kill父进程 3.使用信号机制，子进程退出向父进程发信号。（ps:子进程退出后，父进程来不及调用，此时子进程的状态就是Z，但最终还是调用了wait就没事了）

若子进程还没退出，父进程已经结束，那么就产生了**孤儿进程**。孤儿进程没有什么危害，因为最终孤儿进程会被托付给 init 进程，init会持续调用wait。

**sleep、wakeup与条件变量都属于线程同步机制**，都是线程在等待某个条件满足时，进入睡眠，然后在条件满足时被唤醒。但是还是有些区别在其中：

* 前者通常用于**等待某个事件**的发生，后者**通过一个变量**来判断是否满足条件。
* 正因为后者有了这个变量，因此**条件变量和互斥锁通常一起使用**，互斥锁防止多个线程同时访问条件变量。
* 前者的状态锁有时是某个事件，有时是某个进程的锁（wait、exit）

**lost wakeup**就是指当进程p1在释放掉锁之后准备进入sleep时，发生了切换，另一个线程p2获取到了释放调用的状态锁，然后做某个事件，然后wakeup在这个事件上睡眠的所有线程。但此刻应该睡眠的线程p1还没有进入到sleep中，因此wakeup什么都没有唤醒。等返回到p1，进入睡眠等待唤醒，但是此刻唤醒已经过去了，所以是lost wakeup。

kill系统调用只是将进程的killed设置为1，并且如果该进程正在SLEEPING调整为RUNNABLE，在某个定时器中断或者切换到这个进程时，调用exit退出。

***

## Lab 8：

这个Lab的主题是Locks, 一共两个实验，第一个是memory allocator. 是说原先设计的内存分配是有一个大锁在分配和释放内存时候，因为每个CPU物理和理论上都是一整片大内存，所以上大锁保护。实验要求我们给每一个CPU核拥有自己的freelist，并且自己的freelist有自己的锁，因此，每个CPU核在分配释放内存的时候就不会因为等待锁而耗时。

第二个实验与第一个实验类似，是buffer cache的锁机制修改，原先有一个大锁来管理，现在要求我们利用hash table实现更为细致的管理，每一个bucket都有着自己的锁，因此访问这个buffercache区域就可以多线程并行，如果访问的不是一个bucket里的话。再者就是丢弃原来的双链表实现LRU策略，直接改为时间戳，更为方便。